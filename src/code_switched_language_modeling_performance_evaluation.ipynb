{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49faf8c",
   "metadata": {},
   "source": [
    "# Code switched language modeling performance metric comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc81df4",
   "metadata": {},
   "source": [
    "## Data set preparation\n",
    "### Download raw files\n",
    "We found some twiter corpuses in this repository: https://github.com/divamgupta/mtl_girnet/tree/master/data_prep/data_cm_senti\n",
    "- code switched English and Spanish: https://github.com/divamgupta/mtl_girnet/blob/master/data_prep/data_cm_senti/cs-corpus-with-tweets_test.txt\n",
    "- code switched English and Spanish: https://github.com/divamgupta/mtl_girnet/blob/master/data_prep/data_cm_senti/cs-corpus-with-tweets_train.txt\n",
    "- Spanish: https://github.com/divamgupta/mtl_girnet/blob/master/data_prep/data_cm_senti/1600_tweets_dev_complete.txt\n",
    "- https://github.com/divamgupta/mtl_girnet/blob/master/data_prep/data_cm_senti/twitter4242.txt\n",
    "\n",
    "### Preprocess raw corpuses\n",
    "- remove prefix and trailing information from tweets\n",
    "- combine the training and test set of the code switched data\n",
    "- create three new line seperated lists of tweets:\n",
    " - codes_switched_spanish_english_tweets.txt\n",
    " - spanish_tweets.txt\n",
    " - english_tweets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6729948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "# conda install -c conda-forge openai\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c396ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set API key by copying and pasting it here, do not commit the key to source control\n",
    "\n",
    "open_ai_key = '' # sk-some_long_string\n",
    "\n",
    "openai.api_key = open_ai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fcbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "word_position_to_predict = -3 # many of the english tweets end with a url, we want to predict a word\n",
    "gpt3_engine = 'davinci'\n",
    "temperature = 0.9\n",
    "max_tokens = 10\n",
    "top_p = 1\n",
    "frequency_penalty=1\n",
    "presence_penalty=1\n",
    "number_of_trials = 100  # this is the number of records to try from each corpus\n",
    "\n",
    "settings_dict = {\n",
    "    'word_position_to_predict': word_position_to_predict,\n",
    "    'gpt3_engine': gpt3_engine,\n",
    "    'temperature': temperature,\n",
    "    'max_tokens': max_tokens,\n",
    "    'top_p': top_p,\n",
    "    'frequency_penalty': frequency_penalty,\n",
    "    'presence_penalty': presence_penalty,\n",
    "    'number_of_trials': number_of_trials\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37351fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Waiting for my momma to get home y nada :(',\n",
      " 'Me dos!!! Y fuck a los que no lea guste lol',\n",
      " \"& then my mom wouldn't like her cause she's not my boo! & My sister would \"\n",
      " 'rat me out too!!! Segun muy Best Cu帽adas ellas! Lol']\n"
     ]
    }
   ],
   "source": [
    "# read in the tweet data sets\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_tweets(filename)->list:\n",
    "    tweets = []\n",
    "    with open(filename, 'r', encoding='UTF-8') as file:\n",
    "        while (line := file.readline().rstrip()):\n",
    "            tweets.append(line)\n",
    "    return tweets\n",
    "\n",
    "\n",
    "file_path = '../data/preprocessed_corpuses/'\n",
    "english_tweets = get_tweets(file_path + 'english_tweets.txt')[:number_of_trials]\n",
    "spanish_tweets = get_tweets(file_path + 'spanish_tweets.txt')[:number_of_trials]\n",
    "codes_switched_spanish_english_tweets = get_tweets(file_path + 'code_switched_spanish_english_tweets.txt')[:number_of_trials]\n",
    "\n",
    "pprint(codes_switched_spanish_english_tweets[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f932ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Input = namedtuple('Input', ['original_text', \n",
    "                              'prompt', \n",
    "                              'start',\n",
    "                              'stop',\n",
    "                              'actual_next_word'\n",
    "                              ])\n",
    "\n",
    "GPT_response = namedtuple('GPT_response', ['gpt3_response',\n",
    "                              'predicted_next_word',\n",
    "                              'predicted_actual_match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d932aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_formatting(word: str) -> str:\n",
    "    word = word.lower()\n",
    "    word = word.strip()\n",
    "    word = word.replace('.', '')\n",
    "    word = word.replace(',', '')\n",
    "    word = word.replace('-', '')\n",
    "    word = word.replace('_', '')\n",
    "    word = word.replace('!', '')\n",
    "    word = word.replace('?', '')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7ce742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call GPT3\n",
    "\n",
    "def gpt3_predict_next_word(gpt3_input: Input) -> GPT_response:\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "      engine=gpt3_engine,\n",
    "      prompt=gpt3_input.prompt,\n",
    "      temperature=temperature,\n",
    "      max_tokens=max_tokens,\n",
    "      top_p=top_p,\n",
    "      frequency_penalty=frequency_penalty,\n",
    "      presence_penalty=presence_penalty,\n",
    "      stop=[gpt3_input.start, '\\n'],\n",
    "    )\n",
    "    \n",
    "#     print(response)\n",
    "    \n",
    "    predicted_next_word = response['choices'][0]['text'].strip().split(' ')[0]\n",
    "    \n",
    "    is_match = strip_formatting(gpt3_input.actual_next_word) == strip_formatting(predicted_next_word)\n",
    "    \n",
    "    return GPT_response(response, predicted_next_word, is_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4336bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " I wana see the vid Kyan \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @Mrhilton1985 Welcome to Twitter xx \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " #4WordsOnObamasHand Don't Say The N-Word \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Phil Collins- You Can't Hurry Love \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Equipped a Gamma Mittens. #epicpetwars http://www.epicpetwars.com \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " The Newest Member of _________________! #yehbuddy \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " oh my goodness I'm emo \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " RT @katyperry: I ? New York! \n",
      "\n",
      "[(Input(original_text='?RT @justinbiebcr: The bigger the better....if you know what I mean ;)', prompt='?RT @justinbiebcr: The bigger the better....if you know what', start='?RT', stop='what', actual_next_word='I'),\n",
      "  GPT_response(gpt3_response=<OpenAIObject text_completion id=cmpl-6HNqiSUc19gv7PrBu4pxcU0cqZYaB at 0x7fdc1005f3b0> JSON: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" I mean. https://t.co/rc\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1669601272,\n",
      "  \"id\": \"cmpl-6HNqiSUc19gv7PrBu4pxcU0cqZYaB\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 18,\n",
      "    \"total_tokens\": 28\n",
      "  }\n",
      "}, predicted_next_word='I', predicted_actual_match=True)),\n",
      " (Input(original_text='Listening to the \"New Age\" station on @SlackerRadio ? http://slacker.com/r/nqKf', prompt='Listening to the \"New Age\" station on', start='Listening', stop='on', actual_next_word='@SlackerRadio'),\n",
      "  GPT_response(gpt3_response=<OpenAIObject text_completion id=cmpl-6HNqlpjtqNmt4Q9BcRSJ6t9JAF3JU at 0x7fdc1098bea0> JSON: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" the radio, we heard a bit of \\\"The\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1669601275,\n",
      "  \"id\": \"cmpl-6HNqlpjtqNmt4Q9BcRSJ6t9JAF3JU\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 10,\n",
      "    \"total_tokens\": 20\n",
      "  }\n",
      "}, predicted_next_word='the', predicted_actual_match=False))]\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Mates y panquequess \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Matando la tardeeee http://t.co/sCkfEXFqZy \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @dreambig_es Bienvenidos, gracias por seguirnos \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " RT @viirgiiiniaa: @joakin_29  http://t.co/My6Otjvqn4\" \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Graciassss @catalinabenetti \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Hazme una pregunta | http://t.co/3o3BUDsHpx \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Insisto, siempre te voy a querer \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " RT @JovenTumblr: ASCO NIVEL: http://t.co/6NOWGRYjOG \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " ASCO NIVEL: http://t.co/6NOWGRYjOG \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " anda muy mal el ap \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Osea yo flipo \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " A mi vecino  \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " RT @TobbiGedisman: re esta para dormir. \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " RT @all1Dfacts: AYUDA http://t.co/jBRkMtvJAP \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " \"Hay recuerdos que son para siempre.\" \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Exitos para miiiiiii  #GoodVibes \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Te extra帽ooooo  http://t.co/0bqb6iByZi \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @pepo_43 si en serio  \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @temprana09 nooo a mi tampoco &gt;.&lt; \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @ElvisaYM CON PAPETSE @Marcos_Gomez23 @Nuriieta26 @7eyeblink \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Se siente. \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " http://t.co/bjJW7PoEab http://t.co/zPwxXPnxMb http://t.co/JlnNEbQL9c \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " M谩s lindo el Bruno acompa帽andome \n",
      "\n",
      "[(Input(original_text='Salvo en Innovaci贸n y Ambiente, #Puebla fuera del top de la competitividad urbana nacional http://t.co/OdpYaX8ovJ', prompt='Salvo en Innovaci贸n y Ambiente, #Puebla fuera del top de la competitividad', start='Salvo', stop='competitividad', actual_next_word='urbana'),\n",
      "  GPT_response(gpt3_response=<OpenAIObject text_completion id=cmpl-6HNueh4mDJCvIJlO8pMDDh3VO7EVc at 0x7fdc204c6270> JSON: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" #2017 http://bit.ly/2hr\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1669601516,\n",
      "  \"id\": \"cmpl-6HNueh4mDJCvIJlO8pMDDh3VO7EVc\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 25,\n",
      "    \"total_tokens\": 35\n",
      "  }\n",
      "}, predicted_next_word='#2017', predicted_actual_match=False)),\n",
      " (Input(original_text='Cristiano: \"Lo he hecho otras dos o tres veces. Esto es una cuesti贸n de equipo\"', prompt='Cristiano: \"Lo he hecho otras dos o tres veces. Esto es una', start='Cristiano:', stop='una', actual_next_word='cuesti贸n'),\n",
      "  GPT_response(gpt3_response=<OpenAIObject text_completion id=cmpl-6HNuhkacZVfUvJilG4dOgV9uprzqi at 0x7fdc204c66d0> JSON: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" competici\\u00f3n, \\u00bfno? Cuando\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1669601519,\n",
      "  \"id\": \"cmpl-6HNuhkacZVfUvJilG4dOgV9uprzqi\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 24,\n",
      "    \"total_tokens\": 34\n",
      "  }\n",
      "}, predicted_next_word='competici贸n,', predicted_actual_match=False))]\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Todos close and shit \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @crystal_jaimes aver dime? Txt me!! \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " I todo Por Servir Se acava \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @jasminesloud osea me cais bien lmao \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Sherman got all ghetto lmao xD \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " She made me clean la Casa \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @yveettt lol no mames \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Casi mil corazones me an adorado!!!!! \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @_xoxoBecky that's what I'm craving #semeantojo \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " @adaelisabethh dont tripas carnitas , diviertate \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Damm me duele mi cabezota \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " La cerenta de la prima!!!>>>>>>>>>>>> Lmfaooo \n",
      "\n",
      "\n",
      " -- WARNING --\n",
      " The following input is being skipped because it is too short: \n",
      " Lol ya ni yo \n",
      "\n",
      "[(Input(original_text='Waiting for my momma to get home y nada :(', prompt='Waiting for my momma to get home', start='Waiting', stop='home', actual_next_word='y'),\n",
      "  GPT_response(gpt3_response=<OpenAIObject text_completion id=cmpl-6HNxxumPiCmf2F44taMzpZAA36U21 at 0x7fdc109f4130> JSON: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \")\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1669601721,\n",
      "  \"id\": \"cmpl-6HNxxumPiCmf2F44taMzpZAA36U21\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 9,\n",
      "    \"total_tokens\": 10\n",
      "  }\n",
      "}, predicted_next_word=')', predicted_actual_match=False)),\n",
      " (Input(original_text='Me dos!!! Y fuck a los que no lea guste lol', prompt='Me dos!!! Y fuck a los que no', start='Me', stop='no', actual_next_word='lea'),\n",
      "  GPT_response(gpt3_response=<OpenAIObject text_completion id=cmpl-6HNxzhcsrsmdIs5vjA88CG0l6bTZQ at 0x7fdc109f4590> JSON: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" y que parece que no les gusta\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1669601723,\n",
      "  \"id\": \"cmpl-6HNxzhcsrsmdIs5vjA88CG0l6bTZQ\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 9,\n",
      "    \"total_tokens\": 19\n",
      "  }\n",
      "}, predicted_next_word='y', predicted_actual_match=False))]\n",
      "done calling GPT3\n"
     ]
    }
   ],
   "source": [
    "# iterate over the specified number of records from each language corpus and ask GPT3 to predict the next word\n",
    "\n",
    "english_responses = []\n",
    "spanish_responses = []\n",
    "code_switched_spanish_english_responses = []\n",
    "\n",
    "language_tweets_to_responses_iteration_list = [\n",
    "    (english_tweets, english_responses, 'english'),\n",
    "    (spanish_tweets, spanish_responses, 'spanish'),\n",
    "    (codes_switched_spanish_english_tweets, code_switched_spanish_english_responses, 'code_switched_english_spanish')\n",
    "]\n",
    "\n",
    "for language_tweets_to_responses in language_tweets_to_responses_iteration_list:\n",
    "    for tweet in language_tweets_to_responses[0][:number_of_trials]:\n",
    "        tweet_words = tweet.split(' ')\n",
    "        if len(tweet_words) < abs(word_position_to_predict) + 4:\n",
    "            print('\\n', '-- WARNING --\\n','The following input is being skipped because it is too short: \\n', tweet, '\\n')\n",
    "            continue\n",
    "            \n",
    "        gpt3_input = Input(original_text=tweet,\n",
    "                          prompt=' '.join(tweet_words[:word_position_to_predict]),\n",
    "                          start=tweet_words[0],\n",
    "                          stop=tweet_words[word_position_to_predict-1],\n",
    "                          actual_next_word=tweet_words[word_position_to_predict]\n",
    "                         )\n",
    "        gpt3_response = gpt3_predict_next_word(gpt3_input)\n",
    "        language_tweets_to_responses[1].append((gpt3_input, gpt3_response))\n",
    "        time.sleep(2)  # wait for 2 seconds between calls to stay below rate limit of 60 calls per minute\n",
    "\n",
    "    pprint(language_tweets_to_responses[1][:2])\n",
    "\n",
    "print('done calling GPT3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce46018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance(language='english', total_trials=92, num_correct=20, accuracy=0.21739130434782608)\n",
      "Performance(language='spanish', total_trials=77, num_correct=10, accuracy=0.12987012987012986)\n",
      "Performance(language='code_switched_english_spanish', total_trials=87, num_correct=16, accuracy=0.1839080459770115)\n"
     ]
    }
   ],
   "source": [
    "# calculate cumulative accuracy\n",
    "\n",
    "Performance = namedtuple('Performance', ['language', 'total_trials', 'num_correct', 'accuracy'])\n",
    "\n",
    "language_tweets_to_response_to_accuracy = []\n",
    "\n",
    "for language_tweets_to_responses in language_tweets_to_responses_iteration_list:\n",
    "    number_of_responses, number_of_correct_responses = 0,0\n",
    "\n",
    "    for _input, response in language_tweets_to_responses[1]:\n",
    "        number_of_responses += 1\n",
    "        if response.predicted_actual_match:\n",
    "            number_of_correct_responses += 1\n",
    "    \n",
    "    accuracy = number_of_correct_responses / number_of_responses\n",
    "    \n",
    "    language_tweets_to_response_to_accuracy.append(\n",
    "        (language_tweets_to_responses[0], \n",
    "         language_tweets_to_responses[1], \n",
    "         Performance(language_tweets_to_responses[2], number_of_responses, number_of_correct_responses, accuracy)))\n",
    "\n",
    "for results in language_tweets_to_response_to_accuracy:\n",
    "    print(results[2])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1df0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now: 1669601952\n",
      "settings and standard performance results saved\n"
     ]
    }
   ],
   "source": [
    "# save results to disk\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "now = str(int(time.time()))\n",
    "print('now:', now)\n",
    "\n",
    "\n",
    "def create_folder_(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "\n",
    "results_path = '../results/' + now\n",
    "create_folder_(results_path)\n",
    "\n",
    "with open(results_path + '/standard_perfromance.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(language_tweets_to_response_to_accuracy, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "\n",
    "with open(results_path + '/settings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(settings_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print('settings and standard performance results saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2db860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the folder to load, the 'now' from the standard results that we would like to use\n",
    "standard_results_folder = ''  # such as: 1669591982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b18a63a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard results not loaded\n"
     ]
    }
   ],
   "source": [
    "standard_results = dict()\n",
    "\n",
    "if standard_results_folder:\n",
    "    standard_results_path = '../results/' + standard_results_folder + '/standard_perfromance.json'\n",
    "    with open(standard_results_path) as json_file:\n",
    "        standard_results = json.load(json_file)\n",
    "\n",
    "# pprint(standard_results)\n",
    "\n",
    "if standard_results:\n",
    "    print('Standard results loaded from run', standard_results_folder)\n",
    "else:\n",
    "     print('Standard results not loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ef1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
